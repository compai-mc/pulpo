

import threading
import json
import time
import uuid
from typing import Callable, Optional, Dict
from datetime import datetime
from collections import defaultdict

from kafka import KafkaConsumer, KafkaProducer, TopicPartition
from kafka.structs import OffsetAndMetadata
from kafka.errors import CommitFailedError, KafkaError
import time

from pulpo.util.util import require_env
from pulpo.logueador import log

KAFKA_BROKER = require_env("KAFKA_BROKER")
MAX_RETRY_ATTEMPTS = int(require_env("KAFKA_MAX_RETRIES"))
RETRY_BACKOFF_MS = int(require_env("KAFKA_RETRY_BACKOFF_MS"))
COMMIT_INTERVAL_MS = int(require_env("KAFKA_COMMIT_INTERVAL_MS"))
WARNING_CALLBACK_SLOW_SEG = int(require_env("KAFKA_WARNING_CALLBACK_SLOW_SEG"))

class KafkaEventConsumer:
    def __init__(
        self,
        topic: str,
        callback: Callable,
        bootstrap_servers: str = KAFKA_BROKER,
        group_id: Optional[str] = None,
        auto_offset_reset: str = "latest",
        max_retries: int = MAX_RETRY_ATTEMPTS,
        retry_backoff_ms: int = RETRY_BACKOFF_MS,
        enable_dlq: bool = False,
        dlq_topic: Optional[str] = None,
        batch_commit: bool = True,
        commit_interval_ms: int = COMMIT_INTERVAL_MS,
        max_poll_interval_ms: int = 1800000,
        pass_raw_message: bool = False,
    ):
        self.topic = topic
        self.callback = callback
        self.bootstrap_servers = bootstrap_servers
        self.group_id = group_id or f"consumer_{topic}_{uuid.uuid4().hex[:8]}"
        self.auto_offset_reset = auto_offset_reset
        self.max_retries = max_retries
        self.retry_backoff_ms = retry_backoff_ms
        self.enable_dlq = enable_dlq
        self.dlq_topic = dlq_topic or f"{topic}.dlq"
        self.batch_commit = batch_commit
        self.commit_interval_ms = commit_interval_ms
        self.max_poll_interval_ms = max_poll_interval_ms
        self.pass_raw_message = pass_raw_message

        self._consumer: Optional[KafkaConsumer] = None
        self._dlq_producer: Optional[KafkaProducer] = None
        self._thread: Optional[threading.Thread] = None
        self._running = False
        self._shutdown_event = threading.Event()
        self._lock = threading.Lock()

        self._pending_offsets: Dict[TopicPartition, int] = {}
        self._last_commit_time = time.time()

        self.metrics = {
            "messages_processed": 0,
            "messages_failed": 0,
            "messages_retried": 0,
            "messages_sent_to_dlq": 0,
            "poll_errors": 0,
            "processing_errors": defaultdict(int),
        }

        log.info(f"[{self.group_id}] Consumidor inicializado para topic '{self.topic}'")

    # ------------------------------------------------------------------
    # CREACI√ìN
    # ------------------------------------------------------------------

    def _create_consumer(self) -> KafkaConsumer:
        return KafkaConsumer(
            self.topic,
            bootstrap_servers=self.bootstrap_servers,
            group_id=self.group_id,
            enable_auto_commit=False,
            auto_offset_reset=self.auto_offset_reset,
            max_poll_interval_ms=self.max_poll_interval_ms,
            session_timeout_ms=60000,  # 60s - tiempo sin heartbeat antes de rebalance
            heartbeat_interval_ms=3000,  # 3s - frecuencia de heartbeat (1/3 session)
            request_timeout_ms=70000,  # 70s - timeout de requests individuales
            max_poll_records=1,
            value_deserializer=lambda m: m,
            key_deserializer=lambda k: k.decode() if k else None,
        )

    def _create_dlq_producer(self):
        if not self._dlq_producer:
            self._dlq_producer = KafkaProducer(
                bootstrap_servers=self.bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode("utf-8"),
            )

    # ------------------------------------------------------------------
    # PROCESAMIENTO
    # ------------------------------------------------------------------

    def _deserialize(self, raw: bytes):
        try:
            return json.loads(raw.decode())
        except Exception:
            return raw.decode(errors="ignore")

    def _process_with_retry(self, msg, consumer) -> bool:
        data = msg if self.pass_raw_message else self._deserialize(msg.value)

        for attempt in range(1, self.max_retries + 1):
            try:
                self.callback(data)
                self.metrics["messages_processed"] += 1
                if attempt > 1:
                    self.metrics["messages_retried"] += 1
                return True

            except Exception as e:
                self.metrics["processing_errors"][type(e).__name__] += 1

                if attempt < self.max_retries:
                    # Heartbeat durante retry para no perder conexi√≥n
                    backoff = self.retry_backoff_ms / 1000
                    elapsed = 0
                    while elapsed < backoff and self._running:
                        """try:
                            if consumer and not getattr(consumer, "_closed", True):
                                consumer.poll(timeout_ms=0)
                        except Exception:
                            pass"""
                        sleep_chunk = min(1.0, backoff - elapsed)
                        time.sleep(sleep_chunk)
                        elapsed += sleep_chunk
                else:
                    self.metrics["messages_failed"] += 1
                    if self.enable_dlq:
                        self._send_to_dlq(msg, data, e)
                    return False

    def _send_to_dlq(self, msg, data, error):
        try:
            self._create_dlq_producer()
            
            # Serializar data de forma segura
            try:
                safe_data = str(data) if not isinstance(data, (str, int, float, bool, type(None))) else data
            except Exception:
                safe_data = "<no serializable>"
            
            # Env√≠o as√≠ncrono sin flush para no bloquear
            future = self._dlq_producer.send(
                self.dlq_topic,
                value={
                    "topic": msg.topic,
                    "partition": msg.partition,
                    "offset": msg.offset,
                    "data": safe_data,
                    "error": str(error),
                    "error_type": type(error).__name__,
                    "ts": datetime.utcnow().isoformat(),
                },
            )
            # Callback opcional para verificar errores (no bloquea)
            future.add_errback(lambda e: log.error(f"Error enviando a DLQ: {e}"))
            self.metrics["messages_sent_to_dlq"] += 1
        except Exception as e:
            log.error(f"DLQ error: {e}", exc_info=True)

    # ------------------------------------------------------------------
    # COMMITS
    # ------------------------------------------------------------------

    def _should_commit(self):
        if not self.batch_commit:
            return True
        return (time.time() - self._last_commit_time) * 1000 >= self.commit_interval_ms


    def _commit_offsets(self):
        # 1Ô∏è‚É£ Tomar snapshot de offsets bajo lock
        acquired = self._lock.acquire(timeout=2.0)
        if not acquired:
            log.warning("‚ö†Ô∏è Timeout adquiriendo lock para commit, saltando")
            return

        try:
            consumer = self._consumer
            offsets = dict(self._pending_offsets)
        finally:
            self._lock.release()

        # 2Ô∏è‚É£ Validaciones b√°sicas
        if not consumer or getattr(consumer, "_closed", True):
            return

        if not offsets:
            return

        # 3Ô∏è‚É£ CR√çTICO: filtrar offsets inv√°lidos (None o no int)
        invalid_offsets = {
            tp: offset
            for tp, offset in offsets.items()
            if not isinstance(offset, int)
        }

        if invalid_offsets:
            log.error(
                "‚ùå Offsets inv√°lidos detectados, abortando commit: "
                + ", ".join(
                    f"{tp.topic}[{tp.partition}]={offset!r}"
                    for tp, offset in invalid_offsets.items()
                )
            )
            return

        # 4Ô∏è‚É£ Construir commit_data correctamente
        commit_data = {
            tp: OffsetAndMetadata(
                offset + 1,  # Kafka espera el siguiente offset
                "",          # metadata (string, no None)
                -1           # leader_epoch (OBLIGATORIO en kafka-python)
            )
            for tp, offset in offsets.items()
        }

        try:
            # 5Ô∏è‚É£ Commit s√≠ncrono
            consumer.commit(offsets=commit_data)

            offset_info = ", ".join(
                f"{tp.topic}[{tp.partition}]@{offset + 1}"
                for tp, offset in offsets.items()
            )
            log.debug(f"‚úÖ Commit OK: {offset_info}")

            # 6Ô∏è‚É£ Limpiar pending_offsets bajo lock
            acquired = self._lock.acquire(timeout=1.0)
            if acquired:
                try:
                    for tp, committed_offset in offsets.items():
                        current = self._pending_offsets.get(tp)
                        if current is not None and current <= committed_offset:
                            del self._pending_offsets[tp]
                    self._last_commit_time = time.time()
                finally:
                    self._lock.release()

        except CommitFailedError:
            log.warning("‚ö†Ô∏è Rebalance detectado durante commit")

            acquired = self._lock.acquire(timeout=1.0)
            if acquired:
                try:
                    self._pending_offsets.clear()
                finally:
                    self._lock.release()

            return  # salir del commit, NO seguir

        except KafkaError as e:
            log.error(
                f"‚ùå Error Kafka en commit offsets "
                f"(type={type(e).__name__}, repr={repr(e)})",
                exc_info=True
            )
            # NO limpiar offsets ‚Üí se reintentar√°

        except Exception as e:
            log.error(
                f"‚ùå Error inesperado en commit offsets "
                f"(type={type(e).__name__}, repr={repr(e)})",
                exc_info=True
            )
            # NO limpiar offsets ‚Üí se reintentar√°


    # ------------------------------------------------------------------
    # LOOP PRINCIPAL
    # ------------------------------------------------------------------

    def _consume_loop(self):
        reconnect_attempts = 0
        max_reconnects = 5

        while self._running:
            try:
                # Usar timeout en lock para evitar deadlock
                acquired = self._lock.acquire(timeout=1.0)
                if not acquired:
                    log.warning("‚ö†Ô∏è Timeout adquiriendo lock en loop")
                    continue
                
                try:
                    consumer = self._consumer
                finally:
                    self._lock.release()

                if not consumer or getattr(consumer, "_closed", True):
                    log.warning("‚ö†Ô∏è Consumer no disponible, esperando...")
                    self._shutdown_event.wait(1)
                    continue

                # Poll con timeout reducido para responder r√°pido a shutdown
                poll_timeout = 500 if self._running else 100
                try:
                    records = consumer.poll(timeout_ms=poll_timeout)
                except Exception as poll_err:
                    log.error(f"‚ùå Error en poll: {poll_err}")
                    self._shutdown_event.wait(1)
                    continue
                    
                if not records:
                    continue

                reconnect_attempts = 0

                for tp, msgs in records.items():
                    if not self._running:
                        break
                        
                    for msg in msgs:
                        if not self._running:
                            break
                        
                        # ‚ù§Ô∏è Poll de vida - mantiene heartbeat durante procesamiento lento
                        """if consumer and not getattr(consumer, "_closed", True):
                            try:
                                consumer.poll(timeout_ms=0)
                            except Exception:
                                pass"""
                            
                        start_time = time.time()
                        ok = self._process_with_retry(msg, consumer)
                        elapsed = time.time() - start_time
                        
                        # Advertir si callback es muy largo
                        if elapsed > WARNING_CALLBACK_SLOW_SEG:
                            log.warning(f"‚è±Ô∏è Callback largo: {elapsed:.1f}s para offset {msg.offset}")
                        
                        # Actualizar offset CON protecci√≥n
                        acquired = self._lock.acquire(timeout=0.5)
                        if acquired:
                            try:
                                self._pending_offsets[tp] = msg.offset
                            finally:
                                self._lock.release()

                        if ok and self._should_commit():
                            self._commit_offsets()

            except KafkaError as e:
                reconnect_attempts += 1
                log.error(f"Kafka error: {e} (retry {reconnect_attempts}/{max_reconnects})")

                if reconnect_attempts > max_reconnects:
                    log.critical("üõë M√°ximo de reintentos Kafka alcanzado")
                    break

                backoff = min(2 ** reconnect_attempts, 30)
                log.info(f"üîÑ Reconectando en {backoff}s...")
                self._shutdown_event.wait(backoff)

                if not self._running:
                    break

                # Cerrar consumer viejo con timeout
                acquired = self._lock.acquire(timeout=2.0)
                if acquired:
                    try:
                        old_consumer = self._consumer
                        self._consumer = None
                    finally:
                        self._lock.release()
                    
                    # Cerrar FUERA del lock
                    if old_consumer:
                        try:
                            old_consumer.close()
                        except Exception as close_err:
                            log.warning(f"Error cerrando consumer: {close_err}")
                
                # Crear nuevo consumer FUERA del lock
                try:
                    new_consumer = self._create_consumer()
                    
                    acquired = self._lock.acquire(timeout=2.0)
                    if acquired:
                        try:
                            self._consumer = new_consumer
                        finally:
                            self._lock.release()
                        log.info("‚úÖ Consumidor recreado")
                    else:
                        log.error("‚ùå No se pudo asignar nuevo consumer (timeout lock)")
                        new_consumer.close()
                        continue
                    
                    # Esperar asignaci√≥n de particiones
                    if not self.wait_until_assigned(timeout=10):
                        log.warning("‚ö†Ô∏è Timeout esperando asignaci√≥n tras reconexi√≥n")
                except Exception as create_err:
                    log.error(f"‚ùå Error creando consumer: {create_err}")
                    self._shutdown_event.wait(5)

            except Exception as e:
                log.error(f"Error inesperado: {e}", exc_info=True)
                self._shutdown_event.wait(1)

        self._cleanup()

        
    # ------------------------------------------------------------------
    # CONTROL
    # ------------------------------------------------------------------

    def start(self):
        # Validar que realmente est√° running (thread vivo)
        if self._running and self._thread and self._thread.is_alive():
            log.warning(f"[{self.group_id}] Consumidor ya est√° corriendo")
            return
        
        # Si estaba marcado como running pero thread muri√≥, limpiar
        if self._running:
            log.warning(f"[{self.group_id}] Thread muerto detectado, reiniciando...")
            self._running = False

        self._shutdown_event.clear()
        
        # Crear consumer FUERA del lock para no bloquear
        log.info(f"[{self.group_id}] Creando consumidor...")
        new_consumer = self._create_consumer()
        
        acquired = self._lock.acquire(timeout=5.0)
        if not acquired:
            log.error("‚ùå Timeout adquiriendo lock en start()")
            new_consumer.close()
            raise TimeoutError("No se pudo iniciar consumidor (lock timeout)")
        
        try:
            self._consumer = new_consumer
        finally:
            self._lock.release()

        self._running = True
        self._thread = threading.Thread(
            target=self._consume_loop,
            name=f"kafka-consumer-{self.topic}",
            daemon=True,
        )
        self._thread.start()
        log.info(f"[{self.group_id}] Consumidor iniciado")
        

    def stop(self):
        log.info(f"[{self.group_id}] Deteniendo consumidor...")
        self._running = False
        self._shutdown_event.set()  # Despertar cualquier wait
        
        if self._thread:
            self._thread.join(timeout=10)
            if self._thread.is_alive():
                log.warning(f"‚ö†Ô∏è Thread consumidor a√∫n vivo tras 10s")

    def _cleanup(self):
        log.info(f"[{self.group_id}] Iniciando cleanup...")
        
        # Commit final con timeout
        if self._pending_offsets:
            try:
                self._commit_offsets()
            except Exception as e:
                log.error(f"Error en commit final: {e}")
        
        # Cerrar consumer - puede tardar, pero es necesario
        if self._consumer:
            try:
                log.debug("Cerrando consumer...")
                start = time.time()
                self._consumer.close()
                elapsed = time.time() - start
                if elapsed > 3:
                    log.warning(f"‚ö†Ô∏è Consumer.close() tard√≥ {elapsed:.1f}s")
                else:
                    log.debug(f"Consumer cerrado en {elapsed:.1f}s")
            except Exception as e:
                log.error(f"Error cerrando consumer: {e}")
            finally:
                self._consumer = None
        
        # Cerrar DLQ producer
        if self._dlq_producer:
            try:
                self._dlq_producer.flush(timeout=3)
                self._dlq_producer.close(timeout=5)
            except Exception as e:
                log.error(f"Error cerrando DLQ producer: {e}")
            finally:
                self._dlq_producer = None
        
        log.info(f"[{self.group_id}] Cleanup completado")

    # ------------------------------------------------------------------
    # POLL COMPATIBLE
    # ------------------------------------------------------------------

    def poll(self, timeout_ms: int = 1000, wait_ready: bool = False, wait_ready_timeout: int = 3):
        """
        Wrapper compatible con KafkaConsumer.poll()
        Devuelve {TopicPartition: [ConsumerRecord, ...]} o {} en error
        """
        if self._running:
            log.error("‚ùå poll() no permitido mientras el consumer est√° en modo start()")
            return {}

        if wait_ready:
            if not self.wait_until_assigned(timeout=wait_ready_timeout):
                log.error("‚ùå Timeout esperando asignaci√≥n del consumidor")
                self.metrics["poll_errors"] += 1
                return {}

        # Usar timeout en lock para evitar bloqueo
        acquired = self._lock.acquire(timeout=2.0)
        if not acquired:
            log.warning("‚ö†Ô∏è Timeout adquiriendo lock en poll()")
            self.metrics["poll_errors"] += 1
            return {}
        
        try:
            consumer = self._consumer
        finally:
            self._lock.release()

        if not consumer or getattr(consumer, "_closed", True):
            return {}

        try:
            return consumer.poll(timeout_ms=timeout_ms)
        except Exception as e:
            log.error(f"‚ùå Error en poll(): {e}", exc_info=True)
            self.metrics["poll_errors"] += 1
            return {}

    def wait_until_assigned(self, timeout: int = 10) -> bool:
        """Espera hasta que el consumidor tenga particiones asignadas."""
        start = time.time()
        while time.time() - start < timeout:
            # Lectura directa sin lock (asignaci√≥n es at√≥mica)
            consumer = self._consumer
            
            if consumer and not getattr(consumer, "_closed", True):
                try:
                    assignment = consumer.assignment()
                    if assignment:
                        return True
                except Exception:
                    pass
            
            # Usar event con timeout para responder a shutdown
            if self._shutdown_event.wait(0.2):
                return False
                
        return False

    def get_metrics(self):
        return dict(self.metrics)
    
    def is_running(self) -> bool:
        """
        Indica si el consumidor est√° activo.
        API p√∫blica usada por gestor_tareas.
        """
        return bool(self._running and self._thread and self._thread.is_alive())
    
    def health_check(self) -> Dict:
        """
        Retorna el estado de salud del consumidor para monitoreo.
        """
        consumer = self._consumer
        
        # Leer pending_offsets con lock
        acquired = self._lock.acquire(timeout=0.5)
        if acquired:
            try:
                pending_count = len(self._pending_offsets)
            finally:
                self._lock.release()
        else:
            pending_count = -1  # Indica que no se pudo leer
        
        return {
            "running": self.is_running(),
            "thread_alive": self._thread.is_alive() if self._thread else False,
            "consumer_exists": consumer is not None,
            "consumer_closed": getattr(consumer, "_closed", True) if consumer else True,
            "pending_offsets": pending_count,
            "topic": self.topic,
            "group_id": self.group_id,
            "metrics": self.get_metrics(),
        }
